{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "262fc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings, PersistentClient, Collection\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae1700",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b706c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>System Initialization performs those functions...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whenever a power-on reset occurs, System Initi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As part of System Initialization , the Boot RO...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>System Initialization shall [SRS014] initiate ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>System Initialization shall [SRS292] enable an...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  System Initialization performs those functions...  True\n",
       "1  Whenever a power-on reset occurs, System Initi...  True\n",
       "2  As part of System Initialization , the Boot RO...  True\n",
       "3  System Initialization shall [SRS014] initiate ...  True\n",
       "4  System Initialization shall [SRS292] enable an...  True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv('./dataset/PURE_test.csv', usecols=['Requirement', 'Req/Not Req'])\n",
    "test_dataset.rename(columns={'Requirement': 'text', 'Req/Not Req': 'label'}, inplace=True)\n",
    "test_dataset.replace({'label': {'Req': True, 'Not Req': False}}, inplace=True)\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fbdf060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Any operation requiring the user to supply a f...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For any operation where the user is prompted t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When collecting generated output files from HA...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For example, given a transformation language p...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If X.tlp.parsed existed prior to executing the...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Any operation requiring the user to supply a f...  True\n",
       "1  For any operation where the user is prompted t...  True\n",
       "2  When collecting generated output files from HA...  True\n",
       "3  For example, given a transformation language p...  True\n",
       "4  If X.tlp.parsed existed prior to executing the...  True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = pd.read_csv('./dataset/PURE_valid.csv', usecols=['Requirement', 'Req/Not Req'])\n",
    "validation_dataset.rename(columns={'Requirement': 'text', 'Req/Not Req': 'label'}, inplace=True)\n",
    "validation_dataset.replace({'label': {'Req': True, 'Not Req': False}}, inplace=True)\n",
    "validation_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbe012",
   "metadata": {},
   "source": [
    "# Create the ChromaDB Client and Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3ce57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self, embedding_model: HuggingFaceEmbeddings):\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def __call__(self, texts: Documents) -> Embeddings:\n",
    "        return self.embedding_model.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e5c7e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"Qwen/Qwen3-Embedding-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "059c747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = PersistentClient(path=\"./chroma_data\")\n",
    "\n",
    "chroma_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"requirements_collection_qwen3\",\n",
    "    embedding_function=CustomEmbeddingFunction(embedding_model=embedding_model),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "036d7558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['53e30d48-1277-4edd-bd5b-0bd4fb641a60',\n",
       "   '9c7167b4-d011-459e-bba8-43ea45a38476',\n",
       "   'fb71c5e9-e37a-4ec2-8e7b-18445374acb3',\n",
       "   'b1b71423-c739-479d-ab7b-ca955ad817c8',\n",
       "   'bf6acb83-c3ce-44bb-969e-14ebd94b04e5']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['The time critical functions include both control and supervision functions.',\n",
       "   'The two types are long and short timers.',\n",
       "   'This for instance is the case for condition monitoring of components such as gearbox bearings.',\n",
       "   'Monitoring, troubleshooting, and controlling server performance.',\n",
       "   'Monitoring, troubleshooting, and controlling services, ports, and application programming interfaces.']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'is_req': True},\n",
       "   {'is_req': False},\n",
       "   {'is_req': True},\n",
       "   {'is_req': False},\n",
       "   {'is_req': False}]],\n",
       " 'distances': [[0.7377492189407349,\n",
       "   0.780716061592102,\n",
       "   0.8417319059371948,\n",
       "   0.8508152961730957,\n",
       "   0.8523573875427246]]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test query\n",
    "chroma_collection.query(\n",
    "    query_texts=[\"Watchdog timers are used to detect and recover from malfunctions.\"],\n",
    "    n_results=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9c8f1",
   "metadata": {},
   "source": [
    "# Debug: Check Collection Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5addca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 5306\n",
      "\n",
      "Sample metadata: [{'is_req': True}, {'is_req': True}, {'is_req': True}]\n",
      "Sample documents: ['The solution should provide detailed context-sensitive help material for all the possible actions and scenarios on all user interfaces in the application.', 'The help should be accessible to the users both in the offline and online mode.', 'The solution should provide an interface for the user to log any defects or enhancement requests on the application and track thereafter.']\n"
     ]
    }
   ],
   "source": [
    "# Check if the collection has any data\n",
    "collection_count = chroma_collection.count()\n",
    "print(f\"Total documents in collection: {collection_count}\")\n",
    "\n",
    "# If collection has data, check a sample\n",
    "if collection_count > 0:\n",
    "    sample = chroma_collection.peek(limit=3)\n",
    "    print(f\"\\nSample metadata: {sample['metadatas']}\")\n",
    "    print(f\"Sample documents: {sample['documents']}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ WARNING: Collection is EMPTY! You need to run data ingestion first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ead28f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test query: System Initialization performs those functions necessary to transform the hardware consisting of the FCP processors, network elements, and on-board I/O devices into a real time system executing tasks with fault tolerant message exchanges.\n",
      "True label: True\n",
      "\n",
      "Query results:\n",
      "Distances: [0.8391311764717102, 0.9021110534667969, 0.9062187075614929, 0.9077016115188599, 0.9278020858764648]\n",
      "Metadatas: [{'is_req': False}, {'is_req': True}, {'is_req': False}, {'is_req': True}, {'is_req': False}]\n",
      "Documents: ['Identifies what is to be done by the system, what inputs should be transformed to what outputs, and what specific operations are required. ', 'Automatic procedures for detection of communication faults and for managing redun- dancy of system components shall be established. ']\n",
      "\n",
      "Predicted label: False\n",
      "Correct: False\n"
     ]
    }
   ],
   "source": [
    "# Test a single query to see what we actually get back\n",
    "if collection_count > 0:\n",
    "    test_query = test_dataset[\"text\"].iloc[0]\n",
    "    test_label = test_dataset[\"label\"].iloc[0]\n",
    "    \n",
    "    print(f\"Test query: {test_query}\")\n",
    "    print(f\"True label: {test_label}\")\n",
    "    \n",
    "    result = chroma_collection.query(\n",
    "        query_texts=[test_query],\n",
    "        n_results=5,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery results:\")\n",
    "    print(f\"Distances: {result['distances'][0]}\")\n",
    "    print(f\"Metadatas: {result['metadatas'][0]}\")\n",
    "    print(f\"Documents: {result['documents'][0][:2]}\")  # First 2 docs\n",
    "    \n",
    "    # Test the voting function\n",
    "    predicted = get_range_vote_for_query(test_query, chroma_collection, k=5)\n",
    "    print(f\"\\nPredicted label: {predicted}\")\n",
    "    print(f\"Correct: {predicted == test_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3115a",
   "metadata": {},
   "source": [
    "## Function for Determining the Classification Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4e4eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_range_vote_for_query(query: str, collection: Collection, k: int = 5) -> bool:\n",
    "    \"\"\"\n",
    "    Implements Range/Score Voting to classify a query as requirement or not.\n",
    "    Uses similarity-weighted voting from k nearest neighbors to minimize Bayesian Regret.\n",
    "    \n",
    "    Args:\n",
    "        query: The text to classify\n",
    "        collection: ChromaDB collection containing labeled requirements\n",
    "        k: Number of nearest neighbors to retrieve for voting\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if classified as requirement, False otherwise\n",
    "    \"\"\"\n",
    "    # Query the vector database for k nearest neighbors\n",
    "    query_result = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k,\n",
    "    )\n",
    "    \n",
    "    # Extract distances and metadata\n",
    "    distances = query_result[\"distances\"][0]\n",
    "    metadatas = query_result[\"metadatas\"][0]\n",
    "    \n",
    "    # Convert distances to similarity scores (lower distance = higher similarity)\n",
    "    # Using 1/(1+distance) to convert distance to similarity weight\n",
    "    similarities = [1 / (1 + dist) for dist in distances]\n",
    "    \n",
    "    # Accumulate weighted votes for each class\n",
    "    req_score = 0.0\n",
    "    not_req_score = 0.0\n",
    "    \n",
    "    for similarity, metadata in zip(similarities, metadatas):\n",
    "        if metadata[\"is_req\"]:\n",
    "            req_score += similarity\n",
    "        else:\n",
    "            not_req_score += similarity\n",
    "    \n",
    "    # Return the class with higher weighted score\n",
    "    return req_score > not_req_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e93cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(\n",
    "    true_values: list[str], true_labels: list[bool], vector_collection: Collection, k: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate confusion matrix using Range Voting classification.\n",
    "    \n",
    "    Args:\n",
    "        true_values: List of text samples to classify\n",
    "        true_labels: List of ground truth labels\n",
    "        vector_collection: ChromaDB collection to query\n",
    "        k: Number of nearest neighbors for voting (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        Confusion matrix as a 2D array\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(len(true_values)):\n",
    "        predicted_label = get_range_vote_for_query(\n",
    "            query=true_values[i],\n",
    "            collection=vector_collection,\n",
    "            k=k\n",
    "        )\n",
    "        y_true.append(true_labels[i])\n",
    "        y_pred.append(predicted_label)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[True, False])\n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d79596ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_positive: int, true_negative: int, false_positive: int, false_negative: int) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate classification metrics from confusion matrix values.\n",
    "    \n",
    "    Args:\n",
    "        true_positive: Number of true positives\n",
    "        true_negative: Number of true negatives\n",
    "        false_positive: Number of false positives\n",
    "        false_negative: Number of false negatives\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing accuracy, precision, recall, and f1_score\n",
    "    \"\"\"\n",
    "    # Calculate total samples\n",
    "    total = true_positive + true_negative + false_positive + false_negative\n",
    "    \n",
    "    # Accuracy: (TP + TN) / Total\n",
    "    accuracy = (true_positive + true_negative) / total if total > 0 else 0.0\n",
    "    \n",
    "    # Precision: TP / (TP + FP)\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0.0\n",
    "    \n",
    "    # Recall (Sensitivity): TP / (TP + FN)\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0.0\n",
    "    \n",
    "    # F1 Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3188ba45",
   "metadata": {},
   "source": [
    "# Test Dataset Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a0391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 0, TN: 0, FP: 0, FN: 0\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.0000\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 Score:  0.0000\n"
     ]
    }
   ],
   "source": [
    "true_negative, false_positive, false_negative, true_positive = (\n",
    "    get_confusion_matrix(\n",
    "        true_values=test_dataset[\"text\"].tolist(),\n",
    "        true_labels=test_dataset[\"label\"].tolist(),\n",
    "        vector_collection=chroma_collection,\n",
    "        k=1,\n",
    "    )\n",
    "    .ravel()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"TP: {true_positive}, TN: {true_negative}, FP: {false_positive}, FN: {false_negative}\"\n",
    ")\n",
    "\n",
    "# Calculate and display metrics\n",
    "metrics = calculate_metrics(\n",
    "    true_positive, true_negative, false_positive, false_negative\n",
    ")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {metrics['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83bbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
