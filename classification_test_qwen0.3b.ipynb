{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "262fc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings, PersistentClient, Collection\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae1700",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b706c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>System Initialization performs those functions...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whenever a power-on reset occurs, System Initi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As part of System Initialization , the Boot RO...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>System Initialization shall [SRS014] initiate ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>System Initialization shall [SRS292] enable an...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  System Initialization performs those functions...  True\n",
       "1  Whenever a power-on reset occurs, System Initi...  True\n",
       "2  As part of System Initialization , the Boot RO...  True\n",
       "3  System Initialization shall [SRS014] initiate ...  True\n",
       "4  System Initialization shall [SRS292] enable an...  True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv('./dataset/PURE_test.csv', usecols=['Requirement', 'Req/Not Req'])\n",
    "test_dataset.rename(columns={'Requirement': 'text', 'Req/Not Req': 'label'}, inplace=True)\n",
    "test_dataset.replace({'label': {'Req': True, 'Not Req': False}}, inplace=True)\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fbdf060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Any operation requiring the user to supply a f...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For any operation where the user is prompted t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When collecting generated output files from HA...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For example, given a transformation language p...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If X.tlp.parsed existed prior to executing the...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Any operation requiring the user to supply a f...  True\n",
       "1  For any operation where the user is prompted t...  True\n",
       "2  When collecting generated output files from HA...  True\n",
       "3  For example, given a transformation language p...  True\n",
       "4  If X.tlp.parsed existed prior to executing the...  True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = pd.read_csv('./dataset/PURE_valid.csv', usecols=['Requirement', 'Req/Not Req'])\n",
    "validation_dataset.rename(columns={'Requirement': 'text', 'Req/Not Req': 'label'}, inplace=True)\n",
    "validation_dataset.replace({'label': {'Req': True, 'Not Req': False}}, inplace=True)\n",
    "validation_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbe012",
   "metadata": {},
   "source": [
    "# Create the ChromaDB Client and Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3ce57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self, embedding_model: HuggingFaceEmbeddings):\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def __call__(self, texts: Documents) -> Embeddings:\n",
    "        return self.embedding_model.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e5c7e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"Qwen/Qwen3-Embedding-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "059c747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = PersistentClient(path=\"./chroma_data\")\n",
    "\n",
    "chroma_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"requirements_collection_qwen3\",\n",
    "    embedding_function=CustomEmbeddingFunction(embedding_model=embedding_model),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "036d7558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['53e30d48-1277-4edd-bd5b-0bd4fb641a60',\n",
       "   '9c7167b4-d011-459e-bba8-43ea45a38476',\n",
       "   'fb71c5e9-e37a-4ec2-8e7b-18445374acb3',\n",
       "   'b1b71423-c739-479d-ab7b-ca955ad817c8',\n",
       "   'bf6acb83-c3ce-44bb-969e-14ebd94b04e5']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['The time critical functions include both control and supervision functions.',\n",
       "   'The two types are long and short timers.',\n",
       "   'This for instance is the case for condition monitoring of components such as gearbox bearings.',\n",
       "   'Monitoring, troubleshooting, and controlling server performance.',\n",
       "   'Monitoring, troubleshooting, and controlling services, ports, and application programming interfaces.']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'is_req': True},\n",
       "   {'is_req': False},\n",
       "   {'is_req': True},\n",
       "   {'is_req': False},\n",
       "   {'is_req': False}]],\n",
       " 'distances': [[0.7377492189407349,\n",
       "   0.780716061592102,\n",
       "   0.8417319059371948,\n",
       "   0.8508152961730957,\n",
       "   0.8523573875427246]]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test query\n",
    "chroma_collection.query(\n",
    "    query_texts=[\"Watchdog timers are used to detect and recover from malfunctions.\"],\n",
    "    n_results=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3115a",
   "metadata": {},
   "source": [
    "## Function for Determining the Classification Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4e4eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_range_vote_for_query(query: str, collection: Collection, k: int = 5) -> bool:\n",
    "    \"\"\"\n",
    "    Implements a hybrid voting system to classify a query as requirement or not.\n",
    "    First uses majority voting, then falls back to weighted Range Voting on ties.\n",
    "    \n",
    "    Args:\n",
    "        query: The text to classify\n",
    "        collection: ChromaDB collection containing labeled requirements\n",
    "        k: Number of nearest neighbors to retrieve for voting\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if classified as requirement, False otherwise\n",
    "    \"\"\"\n",
    "    # Query the vector database for k nearest neighbors\n",
    "    query_result = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k,\n",
    "    )\n",
    "    \n",
    "    # Extract distances and metadata\n",
    "    distances = query_result[\"distances\"][0]\n",
    "    metadatas = query_result[\"metadatas\"][0]\n",
    "    \n",
    "    # Count votes for majority voting\n",
    "    req_votes = sum(1 for metadata in metadatas if metadata[\"is_req\"])\n",
    "    not_req_votes = len(metadatas) - req_votes\n",
    "    \n",
    "    # If there's a clear majority, return it\n",
    "    if req_votes > not_req_votes:\n",
    "        return True\n",
    "    elif not_req_votes > req_votes:\n",
    "        return False\n",
    "    \n",
    "    # Tie case: use weighted Range Voting based on distances\n",
    "    # Convert distances to similarity scores (lower distance = higher similarity)\n",
    "    # Using 1/(1+distance) to convert distance to similarity weight\n",
    "    similarities = [1 / (1 + dist) for dist in distances]\n",
    "    \n",
    "    # Accumulate weighted votes for each class\n",
    "    req_score = 0.0\n",
    "    not_req_score = 0.0\n",
    "    \n",
    "    for similarity, metadata in zip(similarities, metadatas):\n",
    "        if metadata[\"is_req\"]:\n",
    "            req_score += similarity\n",
    "        else:\n",
    "            not_req_score += similarity\n",
    "    \n",
    "    # Return the class with higher weighted score\n",
    "    return req_score > not_req_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e93cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(\n",
    "    true_values: list[str], true_labels: list[bool], vector_collection: Collection, k: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate confusion matrix using hybrid voting classification.\n",
    "    Uses majority voting first, then weighted Range Voting for ties.\n",
    "    Uses batch querying for efficiency.\n",
    "    \n",
    "    Args:\n",
    "        true_values: List of text samples to classify\n",
    "        true_labels: List of ground truth labels\n",
    "        vector_collection: ChromaDB collection to query\n",
    "        k: Number of nearest neighbors for voting (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (true_positive, false_positive, false_negative, true_negative)\n",
    "    \"\"\"\n",
    "    # Batch query all texts at once\n",
    "    batch_results = vector_collection.query(\n",
    "        query_texts=true_values,\n",
    "        n_results=k,\n",
    "    )\n",
    "    \n",
    "    # Initialize counters\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    # Process each query's results\n",
    "    for i in range(len(true_values)):\n",
    "        distances = batch_results[\"distances\"][i]\n",
    "        metadatas = batch_results[\"metadatas\"][i]\n",
    "        \n",
    "        # Count votes for majority voting\n",
    "        req_votes = sum(1 for metadata in metadatas if metadata[\"is_req\"])\n",
    "        not_req_votes = len(metadatas) - req_votes\n",
    "        \n",
    "        # Determine prediction\n",
    "        if req_votes > not_req_votes:\n",
    "            # Clear majority for requirement\n",
    "            predicted_label = True\n",
    "        elif not_req_votes > req_votes:\n",
    "            # Clear majority for not requirement\n",
    "            predicted_label = False\n",
    "        else:\n",
    "            # Tie case: use weighted Range Voting\n",
    "            similarities = [1 / (1 + dist) for dist in distances]\n",
    "            \n",
    "            req_score = 0.0\n",
    "            not_req_score = 0.0\n",
    "            \n",
    "            for similarity, metadata in zip(similarities, metadatas):\n",
    "                if metadata[\"is_req\"]:\n",
    "                    req_score += similarity\n",
    "                else:\n",
    "                    not_req_score += similarity\n",
    "            \n",
    "            predicted_label = req_score > not_req_score\n",
    "        \n",
    "        actual_label = true_labels[i]\n",
    "        \n",
    "        # Manually calculate confusion matrix values\n",
    "        if predicted_label == True and actual_label == True:\n",
    "            true_positive += 1\n",
    "        elif predicted_label == True and actual_label == False:\n",
    "            false_positive += 1\n",
    "        elif predicted_label == False and actual_label == False:\n",
    "            true_negative += 1\n",
    "        elif predicted_label == False and actual_label == True:\n",
    "            false_negative += 1\n",
    "\n",
    "    return true_positive, false_positive, false_negative, true_negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d79596ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_positive: int, true_negative: int, false_positive: int, false_negative: int) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate classification metrics from confusion matrix values.\n",
    "    \n",
    "    Args:\n",
    "        true_positive: Number of true positives\n",
    "        true_negative: Number of true negatives\n",
    "        false_positive: Number of false positives\n",
    "        false_negative: Number of false negatives\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing accuracy, precision, recall, and f1_score\n",
    "    \"\"\"\n",
    "    # Calculate total samples\n",
    "    total = true_positive + true_negative + false_positive + false_negative\n",
    "    \n",
    "    # Accuracy: (TP + TN) / Total\n",
    "    accuracy = (true_positive + true_negative) / total if total > 0 else 0.0\n",
    "    \n",
    "    # Precision: TP / (TP + FP)\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0.0\n",
    "    \n",
    "    # Recall (Sensitivity): TP / (TP + FN)\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0.0\n",
    "    \n",
    "    # F1 Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3188ba45",
   "metadata": {},
   "source": [
    "# Test Dataset Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d9a0391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 605, TN: 0, FP: 0, FN: 453\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.5718\n",
      "Precision: 1.0000\n",
      "Recall:    0.5718\n",
      "F1 Score:  0.7276\n"
     ]
    }
   ],
   "source": [
    "true_positive, false_positive, false_negative, true_negative = get_confusion_matrix(\n",
    "    true_values=test_dataset[\"text\"].tolist(),\n",
    "    true_labels=test_dataset[\"label\"].tolist(),\n",
    "    vector_collection=chroma_collection,\n",
    "    k=10,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"TP: {true_positive}, TN: {true_negative}, FP: {false_positive}, FN: {false_negative}\"\n",
    ")\n",
    "\n",
    "# Calculate and display metrics\n",
    "metrics = calculate_metrics(\n",
    "    true_positive, true_negative, false_positive, false_negative\n",
    ")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {metrics['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa129f",
   "metadata": {},
   "source": [
    "# Validation Dataset Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff83bbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 182, TN: 0, FP: 0, FN: 73\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.7137\n",
      "Precision: 1.0000\n",
      "Recall:    0.7137\n",
      "F1 Score:  0.8330\n"
     ]
    }
   ],
   "source": [
    "true_positive, false_positive, false_negative, true_negative = get_confusion_matrix(\n",
    "    true_values=validation_dataset[\"text\"].tolist(),\n",
    "    true_labels=validation_dataset[\"label\"].tolist(),\n",
    "    vector_collection=chroma_collection,\n",
    "    k=10,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"TP: {true_positive}, TN: {true_negative}, FP: {false_positive}, FN: {false_negative}\"\n",
    ")\n",
    "\n",
    "# Calculate and display metrics\n",
    "metrics = calculate_metrics(\n",
    "    true_positive, true_negative, false_positive, false_negative\n",
    ")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {metrics['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586eab40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
